# -*- coding: utf-8 -*-
"""AFclassififcation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11DLpQ8oHnZ5L4SGZX4wIYWZrLo4Dxe2K
"""

import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from google.colab import drive
from numpy.lib.function_base import gradient
import scipy
from scipy import stats
from scipy.fftpack import fft,ifft
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV,cross_val_score
from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay

from google.colab import files
uploaded1 = files.upload()

from google.colab import drive
drive.mount('/content/drive')

def get_CRN_point(file_path):
    data_dict = {}
    row_data = []
    time_stamps = []
    blank_raw = []
    with open(file_path) as file:
        for i,row in enumerate(file):
            if (i+1)%22==1:
                t = re.findall(r'\d+\.\d+',row)
                time_stamps.append(t)
            elif (i+1)%22==0:
                continue
            else:
                data = row.split()
                data_vec = np.array([float(element) for element in data])
                row_data.append(data_vec)
        time_stamps = np.array(time_stamps).flatten()
        for j,t in enumerate(time_stamps):
            data_dict[t] = [row_data[j*20:(j+1)*20]]
        
    return row_data,time_stamps,data_dict

# from google drive
row_data_spiral, train_time_stamp,data_dict = get_CRN_point('/content/drive/MyDrive/TeamAlfa/SimulationData/SimulationOutputSpiralWave/CRN_2D_Voltage_spiral_grid.txt')
data = pd.DataFrame.from_dict(data_dict,orient='index')[0]

# from google drive
row_test_spiral, test_time_stamp,test_dict = get_CRN_point('/content/drive/MyDrive/TeamAlfa/SimulationData/SimulationOutputSpiralWave2/CRN_2D_Voltage_spiral2_grid.txt')
test_data = pd.DataFrame.from_dict(test_dict,orient='index')[0]
test_data

# the first data structure we used(1*10 means 10ms voltage value of each probe)
def trans_time(data,t):
  train_data = []
  for loop in range(int(len(data)/t)):
    temp_10 = []
    for j in range(np.shape(data[loop])[0]):
      for k in range(np.shape(data[loop])[1]):
        temp = [data[i][k][j] for i in range(loop*t,(loop+1)*t,1)]
        temp_10.append(temp)
    train_data.append(temp_10)
  return train_data

# transform the dataset according to x-axes index into unit column(20*20*10)
def trans_x_vector(data,t):
  train_data = []
  for loop in range(int(len(data)/t)):
    for j in range(np.shape(data[loop])[0]):
      temp = [data[i][k][j] for i in range(loop*t,(loop+1)*t,1) for k in range(np.shape(data[loop])[1])]    # extract the col of x order by row 
    train_data.append(temp)
  return train_data

# transform the dataset according to x-axes index into unit column(20*20*10)
def trans_x_matrix(data,t):
  train_data = []
  for loop in range(int(len(data)/t)):
    temp_10 = []
    for j in range(np.shape(data[loop])[0]):
      temp = []
      for k in range(np.shape(data[loop])[1]):
        temp.append([data[i][k][j] for i in range(loop*t,(loop+1)*t,1)])    # extract the col of x order by row 
      temp_10.append(temp)
    train_data.append(temp_10)
  return train_data

# transform the dataset according to y-axes index into unit column(20*20*10)
def trans_y_matrix(data,t):
  train_data = []
  for loop in range(int(len(data)/t)):
    temp_10 = []
    for k in range(np.shape(data[loop])[0]):
      temp = []
      for j in range(np.shape(data[loop])[1]):
        temp.append([data[i][k][j] for i in range(loop*t,(loop+1)*t,1)])    # extract the row of y order by col index
      temp_10.append(temp)
    train_data.append(temp_10)
  return train_data

# get train x unit column
tr_data_x_unit_m = trans_x_matrix(data,10)
print(np.shape(tr_data_x_unit_m))

# get test x unit column
te_data_x_unit_m = trans_x_matrix(test_data,10)
print(np.shape(te_data_x_unit_m))

# train y unit column
tr_data_y_unit_m = trans_y_matrix(data,10)
print(np.shape(tr_data_y_unit_m))

# test y unit column
te_data_y_unit_m = trans_y_matrix(test_data,10)
print(np.shape(te_data_y_unit_m))

!pip install scipy

# extract features from original data
def get_features(raw_data):
  features = dict()
  simp_start_ind = []

  for data in raw_data:
    for i in range(np.shape(data)[0]):
      larg_F_freq_temp = []
      rela_F_ampl_temp = []
      F_freq = []
      features.setdefault('max_val',[]).append(np.max(data[i]))
      features.setdefault('min_val',[]).append(np.min(data[i]))
      features.setdefault('ampl_val',[]).append(features['max_val'][-1]-features['min_val'][-1])
      features.setdefault('intensity',[]).append(np.sum(np.absolute(data[i])))

      g = np.gradient(data[i])[0]   # gradient in time axis
      g_cross = np.gradient(data[i])[1]  # gradient across the probe

      min_t1 = 10
      num1 = 0
      for j in range(20):
        for k in range(9):
          if g[j][k]*g[j][k+1]<0:
            num1 += 1
            if k < min_t1:
              min_t1 = k
      features.setdefault('v0_cro_t_num',[]).append(num1)  # the number of crossing 0 through the time
      features.setdefault('fir_0v_time_t',[]).append(min_t1) # the first time crossing through the time

      min_t2 = 10
      num2 = 0
      for k in range(10):
        for j in range(19):
          if g_cross[j][k]*g_cross[j+1][k]<0:
            num2 += 1
            if k < min_t2:
              min_t2 = k

      features.setdefault('v0_cro_num',[]).append(num2)    # the number of crossing 0 value between probes
      features.setdefault('fir_0v_time',[]).append(min_t2)   # the first time crossing between probes

      features.setdefault('max_grid',[]).append(np.max(g))
      features.setdefault('min_grid',[]).append(np.min(g))
      features.setdefault('ampl_grid',[]).append(features['max_grid'][-1]-features['min_grid'][-1])
      features.setdefault('min_grid_time',[]).append(np.argmin(g))
      features.setdefault('max_grid_time',[]).append(np.argmax(g))
      features.setdefault('ampl_grid_time',[]).append(features['max_grid_time'][-1]-features['min_grid_time'][-1])
      int_d = [int(data[i][j][k]) for j in range(20) for k in range(10)]
      #F_freq = np.fft.rfftfreq(data[i].size,data[i])
      for d in data[i]:
        F_freq.append(np.fft.rfft(d))
      for f in F_freq:
        larg_F_freq_temp.append(np.max(f))
      features.setdefault('larg_F_freq',[]).append(np.max(larg_F_freq_temp))
      ab_F = np.absolute(larg_F_freq_temp)
      features.setdefault('larg_F_ampl',[]).append(ab_F)
      features.setdefault('F_sum',[]).append(np.sum(ab_F))
      for f in ab_F:
        rela_F_ampl_temp.append(f/features['F_sum'][-1])
      features.setdefault('rela_F_ampl',[]).append(np.array(rela_F_ampl_temp))
      features.setdefault('mean',[]).append(scipy.stats.describe(data[i])[2])
      features.setdefault('skewness',[]).append(scipy.stats.describe(data[i])[4])
      features.setdefault('kurtosis',[]).append(scipy.stats.describe(data[i])[5])
      features.setdefault('max_time',[]).append(np.max(np.argmax(data[i],axis=1)))
      features.setdefault('min_time',[]).append(np.min(np.argmin(data[i],axis=1)))
      features.setdefault('ampl_time',[]).append(features['max_time'][-1]-features['min_time'][-1])
      var_po_min = np.std(np.array([data[i][j][k] for j in range(20) for k in range(features['min_time'][-1],10)]))
      features.setdefault('var_po_min',[]).append(var_po_min)

  return features

# extract the train dataset's features
tr_data_x_f = pd.DataFrame(get_features(tr_data_x_unit_m))
tr_data_y_f = pd.DataFrame(get_features(tr_data_y_unit_m))

# extract the test dataset's features
te_data_x_f = pd.DataFrame(get_features(te_data_x_unit_m))
te_data_y_f = pd.DataFrame(get_features(te_data_y_unit_m))

def open_PSdata(file_path):
  ms = []
  ps =[]
  row = []
  col = []
  with open(file_path) as f:
      for line in f:
          s = line.split()
          for i, j in enumerate(s):
              if j == 'ms':
                  #print(s[i],s[i+1])
                  ms.append(s[i+1])
              elif j == 'PS' and s[i-1] == 'of':
                  #print(s[i],s[i+1])
                  ps.append(s[i+1])
              elif j == 'row:':
                  #print(s[i],s[i+1])
                  row.append(int(int(s[i+1])/10))
              elif j == 'col:':
                  #print(s[i],s[i+1])
                  col.append(int(int(s[i+1])/10))

  ps = list(map(int,ps))
  labels = []
  xlabels = []
  ylabels = []
  for i in range(len(ms)):
    y_mat = np.zeros((20,20))
    xlab = np.zeros(20)
    ylab = np.zeros(20)
    for j in range(ps[i]):     
        y_mat[row[j+i],col[j+i]] = 1
        xlab[row[j+i]] = 1
        ylab[col[j+i]] = 1
        #print(y_mat)
    labels.append(y_mat)
    xlabels.append(xlab)
    ylabels.append(ylab)

  return labels,xlabels,ylabels,ps

# from google drive
train_labels,tr_xlabels,tr_ylabels,tr_ps  = open_PSdata('/content/drive/MyDrive/TeamAlfa/SimulationData/SimulationOutputSpiralWave/PSdata.txt')
test_labels,te_xlabels,te_ylabels,te_ps  = open_PSdata('/content/drive/MyDrive/TeamAlfa/SimulationData/SimulationOutputSpiralWave2/PSdata.txt')

# transform X-axes train label into one dimension to feed the model
tr_xlabels = [tr_xlabels[i][j] for i in range(np.shape(tr_xlabels)[0]) for j in range(np.shape(tr_xlabels)[1])] 
np.shape(tr_xlabels)

# transform Y-axes train label into one dimension to feed the model
tr_ylabels = [tr_ylabels[i][j] for i in range(np.shape(tr_ylabels)[0]) for j in range(np.shape(tr_ylabels)[1])] 
np.shape(tr_ylabels)

# transform X-axes test label into one dimension to feed the model
te_xlabels = [te_xlabels[i][j] for i in range(np.shape(te_xlabels)[0]) for j in range(np.shape(te_xlabels)[1])] 
np.shape(te_xlabels)

# transform Y-axes test label into one dimension to feed the model
te_ylabels = [te_ylabels[i][j] for i in range(np.shape(te_ylabels)[0]) for j in range(np.shape(te_ylabels)[1])] 
np.shape(te_ylabels)

# transform the features datatype and structure to be able to feed the model
def trans_features(data):
  for i in range(len(data["larg_F_ampl"][0])):
    data[f"larg_F+{i}"] = data["larg_F_ampl"].apply(lambda x:x[i])
    data[f"rela_F+{i}"] = data["rela_F_ampl"].apply(lambda x:x[i])
  for i in range(len(data["mean"][0])):
    data[f"mean+{i}"] = data["mean"].apply(lambda x:x[i])
    data[f"skewness+{i}"] = data["skewness"].apply(lambda x:x[i])
    data[f"kurtosis+{i}"] = data["kurtosis"].apply(lambda x:x[i]) 
  data[f"larg_F_freq_r"] = data["larg_F_freq"].apply(lambda x:x.real)
  data[f"larg_F_freq_i"] = data["larg_F_freq"].apply(lambda x:x.imag)
  data = data.drop(['larg_F_freq','larg_F_ampl','rela_F_ampl','mean', 'skewness','kurtosis'],axis=1)
  
  return data

# transform the train data
x_train = trans_features(tr_data_x_f)
y_train = trans_features(tr_data_y_f)
x_train.describe()

# oversampling the positive data
from imblearn.over_sampling import SMOTE       
sm = SMOTE(random_state=12)
tr_x_resam, tr_xlabels = sm.fit_resample(x_train,tr_xlabels )
tr_y_resam, tr_ylabels = sm.fit_resample(y_train,tr_ylabels )

len(tr_xlabels)

x_test = trans_features(te_data_x_f)
y_test = trans_features(te_data_y_f)

x_train = np.array(x_train)
x_test = np.array(x_test)
y_train = np.array(y_train)
y_test = np.array(y_test)

clf_x = RandomForestClassifier(n_estimators = 5, max_depth = 50 )
clf_x.fit(tr_x_resam, tr_xlabels)
te_pre_x = clf_x.predict(x_test)
te_pre_x

# visualise the matrics
from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay 
cm_x = confusion_matrix(te_xlabels, te_pre_x, labels=clf_x.classes_)
print('The CM is',cm_x)
ConfusionMatrixDisplay.from_predictions(te_xlabels,te_pre_x)
plt.show()

clf_y = RandomForestClassifier(n_estimators = 5, max_depth = 50 )
clf_y.fit(tr_y_resam, tr_ylabels)
te_pre_y = clf_y.predict(y_test)
te_pre_y

cm_y = confusion_matrix(te_ylabels, te_pre_y, labels=clf_y.classes_)
print('The CM is',cm_y )
ConfusionMatrixDisplay.from_predictions(te_ylabels,te_pre_y)
plt.show()

def unionXY(te_ps,te_pre_x,te_pre_y):  
  PS_pic = [0]*len(te_ps)
  for i in range(len(te_ps)):
    if te_ps[i]>0:
      PS_pic[i]=1

  PS_pic_pre = [0]*(int(len(y_test)/20))
  for i in range(int(len(y_test)/20)):
    if 1 in te_pre_y[i*20:(i+1)*20]:
      if 1 in te_pre_x[i*20:(i+1)*20]:
        PS_pic_pre[i] = 1

  cor_num = 0
  for i in range(len(PS_pic)):
    if PS_pic[i] - PS_pic_pre[i] ==0:
     cor_num += 1
  acc = cor_num/len(te_ps)
  return acc

acc = unionXY(te_ps,te_pre_x,te_pre_y)
acc

# tune the parameters using API
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import GridSearchCV,cross_val_score
from sklearn.ensemble import RandomForestClassifier

max_depth = [50,40,30]
n_estimators = range(10,200,12)
param_grid = dict(n_estimators = n_estimators,max_depth = max_depth)
cv = ShuffleSplit(n_splits=1, test_size=0.3, random_state=42)
grid = GridSearchCV(RandomForestClassifier(), param_grid=param_grid, cv=cv, scoring='f1',error_score='raise')
grid.fit(x_train, labels)
print("Best paras:",grid.best_params_)

# tune the parameter manually
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV,cross_val_score
ScoreAll = []
for i in range(10,200,10):
  DT = RandomForestClassifier(n_estimators = i,random_state= 66)     
  score = cross_val_score(DT,x_train,tr_xlabels,cv = 10).mean()
  ScoreAll.append([i,score])

ScoreAll = np.array(ScoreAll)
max_score = np.where(ScoreAll==np.max(ScoreAll[:,1]))[0][0]
print("best para and score",ScoreAll[max_score])
plt.figure(figsize = [20,5])
plt.plot(ScoreAll[:,0],ScoreAll[:,1])
plt.show()

ScoreAll2 = []
for i in range(80,130):
  DT2 = RandomForestClassifier(n_estimators = i,random_state= 66)     
  score2 = cross_val_score(DT2,x_train,tr_xlabels,cv = 10).mean()
  ScoreAll2.append([i,score2])

ScoreAll2 = np.array(ScoreAll2)
max_score2 = np.where(ScoreAll2==np.max(ScoreAll2[:,1]))[0][0]
print("best para and score",ScoreAll2[max_score2])
plt.figure(figsize = [20,5])
plt.plot(ScoreAll2[:,0],ScoreAll2[:,1])
plt.show()

# analytis the importance of features
importances=clf_x.feature_importances_   
indices=np.argsort(importances)[::-1]
for f in range(20):
  print ("%2d) %-*s %f" % (f+1,30,indices[f],importances[indices[f]]) )